{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Analysis Lab\n",
    "\n",
    "### Detect Normal or Abnormal Industrial Valves, Using Transfer Learning Technology upon Google's Pre-Trained Deep Neural Network\n",
    "\n",
    "The use case here is to use drone to provide regular surveillance on remote or dangerous areas, capturing image of industrial equipment like valves, steam the data back for automatic malfunction diagnosis, using machine intelligence. This improves safety and efficiency compared to current human-involved processes, without large investment on fixed sensor infrastructure. The core part of this solution involves advanced image analysis in real world.\n",
    "\n",
    "\n",
    "In this lab, you will carry out a transfer learning example based on Google Inception-v3 image recognition neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### You will learn:\n",
    "* Explore images in customer’s industry.\n",
    "* Reposition a pre-trained deep neural net for new image recognition task.\n",
    "* Perform feature extraction.\n",
    "* Obtain deep feature representation of customer’s original image.\n",
    "* Train a Support Vector Machine for new classification task.\n",
    "* Evaluate results of this transfer learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Explore images in customer’s industry.\n",
    "Let's have a look at the problem regarding real valve images, in 'images' directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Valves:\n",
    "\n",
    "<img align=\"left\" src='images/normal_valve_68.jpg' width=20%>\n",
    "<img align=\"left\" src='images/normal_valve_78.jpg' width=20%>\n",
    "<img align=\"left\" src='images/normal_valve_37.jpg' width=20%>\n",
    "<img align=\"left\" src='images/normal_valve_01.jpg' width=20%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abormal Valves:\n",
    "\n",
    "<img align=\"left\" src='images/abnormal_valve_55.jpg' width=20%>\n",
    "<img align=\"left\" src='images/abnormal_valve_17.jpg' width=20%>\n",
    "<img align=\"left\" src='images/abnormal_valve_76.jpg' width=20%>\n",
    "<img align=\"left\" src='images/abnormal_valve_01.jpg' width=20%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By: Sam Gu\n",
    "\n",
    "\n",
    "May 2017\n",
    "\n",
    "\n",
    "Reference: https://www.kernix.com/blog/image-classification-with-a-pre-trained-deep-neural-network_p11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find out which file directory we are currently in:\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google's Inception-v3 Deep Neural Network Model\n",
    "\n",
    "The pre-trained deep learning model that will be used is Inception-v3. It has been developed by Google and has been trained for the ImageNet Competition.\n",
    "\n",
    "<img align=\"left\" src='https://4.bp.blogspot.com/-TMOLlkJBxms/Vt3HQXpE2cI/AAAAAAAAA8E/7X7XRFOY6Xo/s1600/image03.png' width=70%>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have downloaded this pre-trained Inception model at 'reference/reusable_model' directory. If you want to re-download this model, run: \n",
    "> !python reference/classify_image.py --model_dir reference/reusable_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Reposition a pre-trained deep neural net for new image recognition task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.platform\n",
    "from tensorflow.python.platform import gfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "from sklearn import cross_validation\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model_dir = 'reference/reusable_model/'\n",
    "images_dir = 'images/'\n",
    "list_images = [images_dir+f for f in os.listdir(images_dir) if re.search('jpg|JPG', f)]\n",
    "\n",
    "print('Number of Customer\\'s Images       : %d' % len(list_images))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use TensorFlow, you should define a graph that represents the description of computations. Then these computations will be executed within what is called sessions. If you want to know more about the basics of TensorFlow, you can go to: https://www.tensorflow.org/\n",
    "\n",
    "The following function creates a graph from the graph definition that we just downloaded and that is saved in classify_image_graph_def.pb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_graph():\n",
    "  with gfile.FastGFile(os.path.join(model_dir, 'classify_image_graph_def.pb'), 'rb') as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "    _ = tf.import_graph_def(graph_def, name='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Perform feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the next step is to extract relevant features.\n",
    "\n",
    "To do so, we retrieve the next-to-last layer of the Inception-v3 as a feature vector for each image. Indeed, the last layer of the convolutional neural network corresponds to the classification step: as it has been trained for the ImageNet dataset, the categories that it will be output will not correspond to the categories in the Product Image Classification dataset we are interested in.\n",
    "\n",
    "The output of the next-to-last layer, however, corresponds to features that are used for the classification in Inception-v3. These **deep features** can be useful for **transfer learning** another classification model, so we extract the output of this layer. In TensorFlow, this layer is called **pool_3**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src='reference/note_slide/note_slide_06.JPG' width=45%>\n",
    "<img align=\"left\" src='reference/note_slide/note_slide_07.JPG' width=45%>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define following function to generate deep features corresponding to the output of this next-to-last layer and the labels (**abnormal_valve** & **normal_valve** based on file name) for each customer image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(list_images):\n",
    "  nb_features = 2048 # Deep Features\n",
    "  features = np.empty((len(list_images),nb_features))\n",
    "  labels = []\n",
    "\n",
    "  create_graph()\n",
    "  \n",
    "  with tf.Session() as sess:\n",
    "    next_to_last_tensor = sess.graph.get_tensor_by_name('pool_3:0')\n",
    "    \n",
    "    for ind, image in enumerate(list_images):\n",
    "      if (ind%25 == 0):\n",
    "        print('Processing %s ...' % (image))\n",
    "      if not gfile.Exists(image):\n",
    "        tf.logging.fatal('File does not exist %s', image)\n",
    "          \n",
    "      image_data = gfile.FastGFile(image, 'rb').read()\n",
    "      predictions = sess.run(next_to_last_tensor,\n",
    "                             {'DecodeJpeg/contents:0': image_data})\n",
    "      features[ind,:] = np.squeeze(predictions)\n",
    "      labels.append(re.split('_\\d+',image.split('/')[1])[0]) # Naming Convention: Class Label + '_' + Digits + .jpg|JPG\n",
    "          \n",
    "    print('')\n",
    "    print('Processing Completed !')\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Obtain deep feature representation of customer’s original image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute deep feature generation for each valve image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features,labels = extract_features(list_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('Number of Images                   : %d' % len(features))\n",
    "print('Unique Image Classess (Labels)     : %s' % list(set(labels)))\n",
    "print('Number of Deep Features per Images : %d [Question here: Why 2048?]' % len(features[0]))\n",
    "print('An Image\\'s Deep Features           : %s' % features[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then the features and labels are saved, so they can be used without re-running this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(features, open('reference/features', 'wb'))\n",
    "pickle.dump(labels, open('reference/labels', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Train a Support Vector Machine for new classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification based on the Deep Features extracted using TensorFlow\n",
    "\n",
    "We will now use the deep features that we just computed with TensorFlow to train a new classifier on the valve images. Another strategy could be to re-train the last layer of the deep neural net in TensorFlow: https://www.tensorflow.org/tutorials/image_retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare training and test datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pickle.load(open('reference/features'))\n",
    "labels = pickle.load(open('reference/labels'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use 80% of the data as the training set and 20% as the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = cross_validation.train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the images with a Linear Support Vector Machine (SVM):\n",
    "We chose to use a linear SVM to classify the images into the 2 categories using the features computed with TensorFlow. We used the LinearSVC implementation with the default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LinearSVC(C=1.0, loss='squared_hinge', penalty='l2',multi_class='ovr')\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate results of this transfer learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Model Performance using Accuracy Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Accuracy: {0:0.1f}%\".format(accuracy_score(y_test, y_pred)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of Model Performance using Confusion Matrix:\n",
    "\n",
    "<img style=\"float: left;\" width=\"60%\" src=\"https://i.ytimg.com/vi/AOIkPnKu0YA/maxresdefault.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "#Calculate Confusion matrix\n",
    "cmlabels = list(set(labels))\n",
    "cm = confusion_matrix(y_pred, y_test, cmlabels)\n",
    "print('Confusion matrix of the classifier')\n",
    "print(cm)\n",
    "\n",
    "# Visualize Confusion matrix\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "cax = ax.matshow(cm)\n",
    "plt.title(u'Confusion matrix of the classifier', fontsize=16)\n",
    "fig.colorbar(cax)\n",
    "ax.set_xticklabels([''] + cmlabels)\n",
    "ax.set_yticklabels([''] + cmlabels)\n",
    "plt.xlabel(u'Actual Class')\n",
    "plt.ylabel(u'Predicted Class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
